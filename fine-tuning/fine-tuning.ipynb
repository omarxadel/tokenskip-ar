{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b462ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/omarxadel/camel_tools.git\n",
    "!camel_data -i all\n",
    "!pip install accelerate peft bitsandbytes transformers trl unsloth unsloth_zoo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98db38ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --no-deps vllm\n",
    "\n",
    "\n",
    "import requests\n",
    "import re\n",
    "\n",
    "# vLLM requirements - vLLM breaks Colab due to reinstalling numpy\n",
    "f = requests.get(\"https://raw.githubusercontent.com/vllm-project/vllm/refs/heads/main/requirements/common.txt\").content\n",
    "with open(\"vllm_requirements.txt\", \"wb\") as file:\n",
    "    file.write(re.sub(rb\"(transformers|numpy|xformers)[^\\n]{1,}\\n\", b\"\", f))\n",
    "\n",
    "    \n",
    "!pip install -r vllm_requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6879dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import time\n",
    "from transformers import GenerationConfig\n",
    "import json\n",
    "from peft import LoraConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa0c0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 2048\n",
    "lora_rank = 32\n",
    "model_name=\"Omartificial-Intelligence-Space/Arabic-DeepSeek-R1-Distill-8B\"\n",
    "\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=model_name,\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=None,\n",
    "    load_in_4bit=True,\n",
    "    fast_inference = True,\n",
    "    max_lora_rank = lora_rank,\n",
    "    gpu_memory_utilization = 0.6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "527e7b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset('MBZUAI/ArabicMMLU', 'All')['test']\n",
    "train_testvalid = dataset.train_test_split(0.1)\n",
    "test_valid = train_testvalid['test'].train_test_split(0.5)\n",
    "dataset = DatasetDict({\n",
    "    'train': train_testvalid['train'],\n",
    "    'test': test_valid['test'],\n",
    "    'valid': test_valid['train']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab1c6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_example(example):\n",
    "    chat_template = \"\"\"Below are some Multiple Choice Questions. Write responses in Arabic language only that appropriately complete each request in a valid, parsable JSON format with two attributes, one will be \"reasoning\" which is your thought process, \n",
    "        the other is the \"solution\" that has only a letter (a, b, c or d) in English, which represents the option you chose for the solution based on the options provided in the question.\n",
    "        ### Question:\n",
    "        {INPUT}\n",
    "        \n",
    "        ### Options:\n",
    "        {OPTIONS}\n",
    "        \n",
    "        ### Solution JSON:\n",
    "    \"\"\"\n",
    "    question = example['Question'] + (' ' + example['Context'] if example.get('Context') else '')\n",
    "    options = [\n",
    "        example[\"Option 1\"],\n",
    "        example[\"Option 2\"],\n",
    "        example[\"Option 3\"],\n",
    "        example[\"Option 4\"],\n",
    "    ]\n",
    "    answer = example[\"Answer Key\"]\n",
    "\n",
    "    instruction = f\"{question}\\n\"\n",
    "    options_str = \"\"\n",
    "    for i, opt in enumerate(options):\n",
    "        options_str += f\"{chr(97+i)}. {opt}\\n\"  # a, b, c, d\n",
    "\n",
    "    prompt = chat_template.replace(\"{INPUT}\", instruction)\n",
    "    prompt = prompt.replace(\"{OPTIONS}\", options_str)\n",
    "    return {'input_text': prompt}\n",
    "\n",
    "formatted_dataset = dataset.map(format_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0caec78c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from camel_tools.morphology.database import MorphologyDB\n",
    "from camel_tools.morphology.analyzer import Analyzer\n",
    "from camel_tools.tokenizers.morphological import MorphologicalTokenizer\n",
    "from camel_tools.disambig.mle import MLEDisambiguator\n",
    "\n",
    "mle_msa = MLEDisambiguator.pretrained('calima-msa-r13')\n",
    "morph_tokenizer = MorphologicalTokenizer(disambiguator=mle_msa, scheme='atbtok')\n",
    "\n",
    "class CustomArabicTokenizer:\n",
    "    def __init__(self, base_tokenizer, morph_tokenizer):\n",
    "        self.base_tokenizer = base_tokenizer\n",
    "        self.morph_tokenizer = morph_tokenizer\n",
    "\n",
    "    def __call__(self, text, **kwargs):\n",
    "        morph_tokens = self.camel_morph_tokenize(text)\n",
    "        morph_text = ' '.join(morph_tokens)\n",
    "        return self.base_tokenizer(morph_text, **kwargs)\n",
    "\n",
    "    def camel_morph_tokenize(self, text):\n",
    "        if isinstance(text, list):\n",
    "            text = ' '.join(text)\n",
    "        elif not isinstance(text, str):\n",
    "            raise TypeError(\"Input text must be a string or a list of strings.\")\n",
    "        words = text.split() \n",
    "        tokenized_words = self.morph_tokenizer.tokenize(words)\n",
    "        return tokenized_words\n",
    "\n",
    "    def tokenize(self, text, **kwargs):\n",
    "        morph_tokens = self.camel_morph_tokenize(text)\n",
    "        morph_text = ' '.join(morph_tokens)\n",
    "        return self.base_tokenizer.tokenize(morph_text, **kwargs)\n",
    "\n",
    "    def decode(self, token_ids, **kwargs):\n",
    "        return self.base_tokenizer.decode(token_ids, **kwargs)\n",
    "\n",
    "\n",
    "custom_tokenizer = CustomArabicTokenizer(tokenizer, morph_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4148ee1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    result = tokenizer(str(examples[\"input_text\"]),truncation=True,   \n",
    "                       max_length=512, return_overflowing_tokens=True)\n",
    "\n",
    "    sample_map = result.pop(\"overflow_to_sample_mapping\")\n",
    "    for key, values in examples.items():\n",
    "        result[key] = [values[i] for i in sample_map]\n",
    "    return result\n",
    "\n",
    "tokenized_datasets = formatted_dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97893730",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "answer_to_index = {'A': 0, 'B': 1, 'C': 2, 'D': 3, '-1': 4}\n",
    "\n",
    "class FinalAnswerLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FinalAnswerLoss, self).__init__()\n",
    "        self.loss_fn = nn.BCELoss()\n",
    "\n",
    "    def forward(self, model_output, correct_answer):\n",
    "        print(model_output, correct_answer)\n",
    "        model_output_string = custom_tokenizer.decode(model_output, skip_special_tokens=True)\n",
    "        predicted_answer = self.extract_final_answer(model_output_string)\n",
    "\n",
    "        predicted_idx = torch.tensor([answer_to_index[predicted_answer]], dtype=torch.long)\n",
    "        correct_idx = torch.tensor([answer_to_index[correct_answer]], dtype=torch.long)\n",
    "\n",
    "        print(predicted_answer, correct_answer)\n",
    "        is_correct = 1.0 if predicted_idx == correct_idx else 0.0\n",
    "\n",
    "        prediction = torch.tensor([is_correct], dtype=torch.float32)\n",
    "        target = torch.tensor([1.0], dtype=torch.float32)  \n",
    "\n",
    "        # Compute BCE loss\n",
    "        loss = self.loss_fn(prediction, target)\n",
    "        return loss\n",
    "    \n",
    "    def extract_final_answer(self, model_output):\n",
    "        # Match the block that starts after \"### Solution JSON:\"\n",
    "        # and extract everything from the first '{' to the matching '}'\n",
    "        match = re.search(r'###\\s*Solution\\s*JSON:\\s*(\\{.*)', model_output, re.DOTALL)\n",
    "        \n",
    "        if not match:\n",
    "            # Try fallback: look for any JSON-like object\n",
    "            match = re.search(r'(\\{[^{}]*\"solution\"\\s*:\\s*\".*?\"[^{}]*\\})', model_output, re.DOTALL)\n",
    "        \n",
    "        if not match:\n",
    "            return '-1' \n",
    "\n",
    "        json_str = match.group(1).strip()\n",
    "        json_str = match.group(1).replace('\\n', ' ').replace('\\r', '') + '}'\n",
    "\n",
    "        try:\n",
    "            parsed = json.loads(json_str)\n",
    "            return parsed.get(\"solution\", \"\").strip().upper()\n",
    "        except Exception as e:\n",
    "            print(model_output)\n",
    "            return '-1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2edd79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_final_answer_loss():\n",
    "    model_output_sample = \"\"\"\n",
    "        Below are some Multiple Choice Questions. Write responses in Arabic language only that appropriately complete each request in a valid, parsable JSON format with two attributes, one will be \\\"reasoning\\\" which is your thought process, \\n    the other is the \\\"solution\\\" that has only a letter (a, b, c or d) in English, which represents the option you chose for the solution based on the options provided in the question.\\n\\n### Question:\\nالمقاومة الكهربائية لمصباح مكتوب عليه 220 فولت ، 100 واط  هي\\n\\n\\n### Options:\\na. 220 أوم\\nb. 202 أوم\\nc. 100 أوم\\nd. 484 أوم\\n\\n\\n### Solution JSON:\\n{\\n    \\\"reasoning\\\": \\\"المقاومة الكهربائية لمصباح مكتوب عليه 220 فولت هي 100 واط. لجدول المقاومة الكهربائية، نستخدم صيغة R = V^2 / P، حيث R هي المقاومة، V هو التوتر الكهربائي، وP هو القوة الكهربائية. نستبدل القيم المعطاة: V = 220 فولت، P = 100 واط. نستبدلها في الصيغة: R = (220)^2 / 100 = 484 أوم. لذلك، المقاومة الكهربائية للمصباح هي 484 أوم.\\\",\\n    \\\"solution\\\": \\\"d\\\"\n",
    "    \"\"\"\n",
    "    model_output = custom_tokenizer(model_output_sample)['input_ids']\n",
    "\n",
    "    correct_answer = 'A'\n",
    "    wrong_answer = 'D'\n",
    "    loss_fn = FinalAnswerLoss()\n",
    "    computed_loss = loss_fn(model_output, correct_answer)\n",
    "    print(computed_loss)\n",
    "\n",
    "# Run the test\n",
    "test_final_answer_loss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9028d155",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Trainer\n",
    "\n",
    "class CustomTrainer(Trainer):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "        # Forward pass\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.get('logits')\n",
    "\n",
    "        # Extract the correct answer from inputs\n",
    "        correct_answer = inputs.get('Answer Key')\n",
    "\n",
    "        # Compute custom loss\n",
    "        loss_fct = FinalAnswerLoss()\n",
    "        loss = loss_fct(logits, correct_answer)\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00ef7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return {'accuracy': accuracy_score(labels, predictions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b931b970",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    ")\n",
    "\n",
    "trainer = CustomTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets['train'],\n",
    "    eval_dataset=tokenized_datasets['valid'],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ce2825",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
